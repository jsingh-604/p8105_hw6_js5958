---
title: "P8105 HW6 | Jagjit Singh | UNI: JS5958 "
output: github_document
---

```{r}
library(tidyverse)
library(viridis)
library(dplyr)
library(purrr)
library(broom)
```


### Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 


# Problem 2

Loading homicide data and cleaning it. Also creating a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved:
```{r}
df =
  read.csv("./data/homicide-data.csv", na = c("", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, ', ', state),
    resolved = case_when(
           disposition =="Closed without arrest" ~ 0,
           disposition =="Open/No arrest" ~ 0,
           disposition =="Closed by arrest" ~ 1
         ))
```

Removing cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also removing Tulsa, AL:
```{r}
df <- df %>%
  subset(city_state!="Phoenix, AZ" & city_state!="Kansas City, MO" & 
  city_state!="Tulsa, AL" & city_state!="Dallas, TX")
```

Filtering for victim_race = white or black
```{r}
df <- df %>%
  filter(victim_race == "White" | victim_race == "Black")
```

```{r}
df <- df %>%
  mutate(victim_age = as.numeric(victim_age))
```

For the city of Baltimore MD, using the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors:
```{r}

baltimore_data <- df %>%
  filter(city_state == "Baltimore, MD")  %>% 
  select(resolved, victim_age, victim_race, victim_sex)

dat = baltimore_data %>%
  mutate(victim_sex =  as.factor(victim_sex),
         victim_race = as.factor(victim_race))
baltimore_model <- glm(resolved ~ victim_age + victim_sex + victim_race, 
                       data = dat, family = "binomial")

baltimore_model %>% 
  broom::tidy(conf.int = T) %>% 
  mutate(OR = exp(estimate),
         CI_lower = exp(exp(conf.low)),
         CI_upper = exp(exp(conf.high)),
         p_val = rstatix::p_format(p.value, digits = 2)) %>% 
  select(term, OR, CI_lower,CI_upper, p_val) %>% 
  knitr::kable(digits = 3, align = "lccc", 
               col.names = c("Term", "Estimated adjusted OR", "CI lower bound", 
                             "CI upper bound", "p-value"))
```

From the model summary we can see that, for the city of Baltimore homicides with female victims have a higher chance (more likely) of being resolved versus homicides with male victims, while controlling for all other variables. 

Running a glm for each of the cities in the dataset, and extracting the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims:
```{r}
df_cities = df %>% 
  nest(data = -city_state) %>%
  mutate(
    models = map(data, ~glm(resolved ~ victim_race + victim_sex + victim_age, 
                            data = ., family = binomial(link = "logit"))),
    results = map(models, ~broom::tidy(.x, conf.int = T))) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(OR = exp(estimate),CI_lower = exp(conf.low),CI_upper = exp(conf.high),
    p_val = rstatix::p_format(p.value, digits = 3)
  ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, CI_lower, OR, CI_upper, p_val) 

df_cities%>% 
  knitr::kable(digits = 3, align = "llccc", col.names = 
                 c("City", "Estimated adjusted OR", "CI lower bound", 
                   "CI upper bound", "p-value"))
```

Creating a plot that shows the estimated ORs and CIs for each city:
```{r}
df_cities %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) +
  geom_point(size = 1, aes(colour = OR)) +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(x = "City, State", y = "Estimated OR with CI")
```
From the plot we can see that for most cities, the estimated odds ratio is less than 1. In-fact for almost half of cities, 1 is still higher than their confidence interval upper-limit. This implies that homicides with female victims have a higher chance (more likely) of being resolved versus homicides with male victims, while controlling for all other variables.

# Problem 3 

Loading data and cleaning the data:
```{r}
df_weight <- 
  read_csv("./data/birthweight.csv") %>%
  janitor::clean_names()

```

Checking for any missing values and computing summary statistics:
```{r}
skimr :: skim(df_weight)
```
From the table we can see that there are no missing values.

### The regression model

To help build the regression model, we will check the correlation of child’s birth weight with the other variables in the data set. This would help in building an optimum regression model:
```{r}
library(rstatix)
df_weight %>% 
  cor_mat() %>% 
  cor_gather() %>% 
  filter(var1 %in% c("bwt")) %>% 
  filter(!var2 %in% c("bwt")) %>% 
  ggplot(aes(x = var1, y = var2, fill = cor,label = cor)) + 
  geom_tile(color = "white") +  scale_x_discrete() + geom_text(
  ) + 
  labs(
    x = "Outcome Variable",
    y = "Predictor Variables")
```

From the above correlation plot we can select the variables which are significantly correlated with the child’s birth weight. These variables are: mother’s weight gain during pregnancy (pounds), mother’s height (inches), gestational age in weeks, mother’s weight at delivery (pounds), baby’s head circumference at birth (centimeters), and baby’s length at birth (centimeters).

Converting numeric values to factors wherever it is appropriate based on the encoding:
```{r}
df_weight <-
  df_weight %>%
  mutate(babysex = recode(babysex,'1' = 'male','2' = 'female'),
         babysex = factor(babysex, levels = c('male', 'female')),
         frace = recode(frace,'1' = 'White','2' = 'Black','3' = 'Asian',
                        '4' = 'Puerto Rican','8' = 'Other','9' = 'Unknown'), 
         frace = factor(frace, levels = c('White', 'Black', 'Asian', 
                                          'Puerto Rican', 'Other')),
         malform = recode(malform,'0' = 'absent','1' = 'present'),
         malform = factor(malform, levels = c('absent', 'present')), 
         mrace = recode(mrace,'1' = 'White','2' = 'Black','3' = 'Asian',
                        '4' = 'Puerto Rican','8' = 'Other'), 
         mrace = factor(mrace, levels = c('White', 'Black', 
                                          'Asian', 'Puerto Rican', 'Other')))

```

Now we will build a regression model using our selected predictors:
```{r}
reg_model <- lm(bwt ~ wtgain + mheight + gaweeks + delwt + bhead + blength,  data =df_weight)

reg_model %>%
  broom::tidy() %>%
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 3)
```

Plot of model residuals against fitted values – using add_predictions and add_residuals in making this plot:
```{r}
library(modelr)
library(ggplot2)
df_weight %>% 
  add_predictions(reg_model) %>% 
  add_residuals(reg_model) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point( alpha = 0.5) +
  geom_smooth(method = "lm",
              se = FALSE) +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = "Residuals against Fitted Values")
```

### Comparing our model to two others:

Specifying the models:
```{r}
model_1 <- lm(bwt ~ blength + gaweeks, data = df_weight)

model_2 <- lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex +
                blength*babysex + bhead*blength*babysex,data = df_weight)
```

Make this comparison in terms of the cross-validated prediction error:
```{r}
cv_df <-
  crossv_mc(df_weight, 100) %>% 
  mutate(
    birthweight_model = map(train, ~lm(bwt ~ wtgain + mheight + gaweeks + delwt + bhead + blength, data = .x)),
    model_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_2 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_birthweight = map2_dbl(birthweight_model, test,  ~rmse(model = .x, data = .y)),
    rmse_mod1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_mod2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)))

```

Making the plot:
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(everything(),names_to = "model", values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +labs(x = "Models", y = "RMSE") +
  scale_x_discrete(
    labels = c("Our Model", "Model 1", "Model 2"))
```

From the violin plot we can see that model 1 has the highest RMSE. Our model is the most accurate.
