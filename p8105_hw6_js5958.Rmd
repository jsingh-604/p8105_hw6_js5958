---
title: "p8105_hw6_js5958"
output: github_document
date: "2022-12-03"
---

```{r}
library(tidyverse)
library(viridis)
library(dplyr)
library(purrr)
library(broom)
```


### Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 


# Problem 2
```{r}
df =
  read.csv("./data/homicide-data.csv", na = c("", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, ', ', state),
    resolved = case_when(
           disposition =="Closed without arrest" ~ 0,
           disposition =="Open/No arrest" ~ 0,
           disposition =="Closed by arrest" ~ 1
         ))
```

```{r}
df <- df %>%
  subset(city_state!="Phoenix, AZ" & city_state!="Kansas City, MO" & 
  city_state!="Tulsa, AL" & city_state!="Dallas, TX")
```

```{r}
df = df %>% 
  mutate(
    resolved = case_when(
           disposition =="Closed without arrest" ~ 0,
           disposition =="Open/No arrest" ~ 0,
           disposition =="Closed by arrest" ~ 1),
    victim_age = as.numeric(victim_age))
```

```{r}
df <- df %>%
  filter(victim_race == "White" | victim_race == "Black")
```

```{r}
df <- df %>%
  mutate(victim_age = as.numeric(victim_age))
```

```{r}
baltimore_data <- df %>%
  filter(city_state == "Baltimore, MD")  %>% 
  select(resolved, victim_age, victim_race, victim_sex)
```

```{r}
dat = baltimore_data %>%
  mutate(victim_sex =  as.factor(victim_sex),
         victim_race = as.factor(victim_race))
baltimore_model <- glm(resolved ~ victim_age + victim_sex + victim_race, data = dat, family = "binomial")

baltimore_model %>% 
  broom::tidy(conf.int = T) %>% 
  mutate(OR = exp(estimate),
         CI_lower = exp(exp(conf.low)),
         CI_upper = exp(exp(conf.high)),
         p_val = rstatix::p_format(p.value, digits = 2)) %>% 
  select(term, OR, CI_lower,CI_upper, p_val) %>% 
  knitr::kable(digits = 3, align = "lccc", 
               col.names = c("Term", "Estimated adjusted OR", "CI lower bound", "CI upper bound", "p-value"))
```

```{r}
# Run glm for each city in dataset and extract OR and CI for male vs female victims
cities_fit = df %>% 
  nest(data = -city_state) %>%
  mutate(
    models = map(data, ~glm(resolved ~ victim_race + victim_sex + victim_age, 
                            data = ., family = binomial(link = "logit"))),
    results = map(models, ~broom::tidy(.x, conf.int = T))) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(conf.low),
    CI_upper = exp(conf.high),
    p_val = rstatix::p_format(p.value, digits = 3)
  ) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, CI_lower, OR, CI_upper, p_val) 

cities_fit %>% 
  knitr::kable(digits = 3, align = "llccc", col.names = c("City", "Estimated adjusted OR", "CI lower bound", "CI upper bound", "p-value"))

```
# Problem 3 
```{r}
#Loading Data and cleaning the data
df_weight <- 
  read_csv("./data/birthweight.csv") %>%
  janitor::clean_names()

```

Checking for any missing values and computing summary statistics:
```{r}
skimr :: skim(df_weight)
```
From the table we can see that there are no missing values


Now we will check correlation of weights with the other variables in the dataset. This would help in building an optimum regression model
```{r}
library(rstatix)
df_weight %>% 
  cor_mat() %>% 
  cor_gather() %>% 
  filter(var1 %in% c("bwt")) %>% 
  filter(!var2 %in% c("bwt")) %>% 
  ggplot(aes(x = var1, y = var2, fill = cor,label = cor)) + 
  geom_tile(color = "white") +  scale_x_discrete() + geom_text(
  ) + 
  labs(
    x = "Outcome Variable",
    y = "Predictor Variables")
```

From the above correlation map we can selected the variables which are significantly correlated with weight. The variables we will be using are mother’s weight gain during pregnancy (pounds), gestational age in weeks, mother’s weight at delivery (pounds),  baby’s head circumference at birth (centimeters), baby’s length at birth (centimeters).

Converting numeric value to factor where ever it is appropriate:
```{r}
df_weight <-
  df_weight %>%
  mutate(babysex = recode(babysex,'1' = 'male','2' = 'female'),
         babysex = factor(babysex, levels = c('male', 'female')),
         frace = recode(frace,'1' = 'White','2' = 'Black','3' = 'Asian',
                        '4' = 'Puerto Rican','8' = 'Other','9' = 'Unknown'), 
         frace = factor(frace, levels = c('White', 'Black', 'Asian', 
                                          'Puerto Rican', 'Other')),
         malform = recode(malform,'0' = 'absent','1' = 'present'),
         malform = factor(malform, levels = c('absent', 'present')), 
         mrace = recode(mrace,'1' = 'White','2' = 'Black','3' = 'Asian',
                        '4' = 'Puerto Rican','8' = 'Other'), 
         mrace = factor(mrace, levels = c('White', 'Black', 
                                          'Asian', 'Puerto Rican', 'Other')))

```

# The regression model
```{r}
reg_model <- lm(bwt ~ wtgain + mheight + gaweeks + delwt + bhead + blength,  data =df_weight)
#obtaining a quick summary of the model and for cleaning up the coefficient table using broom
reg_model %>%
  broom::glance()
```

```{r}
reg_model %>%
  broom::tidy() %>%
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 3)
```


Plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot:
```{r}
library(modelr)
library(ggplot2)
df_weight %>% 
  add_predictions(reg_model) %>% 
  add_residuals(reg_model) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point( alpha = 0.5) +
  geom_smooth(method = "lm",
              se = FALSE) +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = "Residuals against Fitted Values")
```
Compareing our model to two others:
```{r}
model_1 <- lm(bwt ~ blength + gaweeks, data = df_weight)

model_2 <- lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex +
                blength*babysex + bhead*blength*babysex,data = df_weight)
```

```{r}
cv_df <-
  crossv_mc(df_weight, 100) %>% 
  mutate(
    birthweight_model = map(train, ~lm(bwt ~ wtgain + mheight + gaweeks + delwt + bhead + blength, data = .x)),
    model_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    model_2 = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_birthweight = map2_dbl(birthweight_model, test,  ~rmse(model = .x, data = .y)),
    rmse_mod1 = map2_dbl(model_1, test, ~rmse(model = .x, data = .y)),
    rmse_mod2 = map2_dbl(model_2, test, ~rmse(model = .x, data = .y)))

```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(everything(),names_to = "model", values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() +labs(
    x = "Models",
    y = "RMSE (root mean square error)"
  ) +
  scale_x_discrete(
    labels = c("My Model", "Given Model 1", "Given Model 2")
  )

```

